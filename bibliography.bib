
@misc{bommasani_opportunities_2022,
	title = {On the Opportunities and Risks of Foundation Models},
	url = {http://arxiv.org/abs/2108.07258},
	doi = {10.48550/arXiv.2108.07258},
	abstract = {{AI} is undergoing a paradigm shift with the rise of models (e.g., {BERT}, {DALL}-E, {GPT}-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.},
	number = {{arXiv}:2108.07258},
	publisher = {{arXiv}},
	author = {Bommasani, Rishi and Hudson, Drew A. and Adeli, Ehsan and Altman, Russ and Arora, Simran and Arx, Sydney von and Bernstein, Michael S. and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch, Shyamal and Card, Dallas and Castellon, Rodrigo and Chatterji, Niladri and Chen, Annie and Creel, Kathleen and Davis, Jared Quincy and Demszky, Dora and Donahue, Chris and Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and Etchemendy, John and Ethayarajh, Kawin and Fei-Fei, Li and Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha, Neel and Hashimoto, Tatsunori and Henderson, Peter and Hewitt, John and Ho, Daniel E. and Hong, Jenny and Hsu, Kyle and Huang, Jing and Icard, Thomas and Jain, Saahil and Jurafsky, Dan and Kalluri, Pratyusha and Karamcheti, Siddharth and Keeling, Geoff and Khani, Fereshte and Khattab, Omar and Koh, Pang Wei and Krass, Mark and Krishna, Ranjay and Kuditipudi, Rohith and Kumar, Ananya and Ladhak, Faisal and Lee, Mina and Lee, Tony and Leskovec, Jure and Levent, Isabelle and Li, Xiang Lisa and Li, Xuechen and Ma, Tengyu and Malik, Ali and Manning, Christopher D. and Mirchandani, Suvir and Mitchell, Eric and Munyikwa, Zanele and Nair, Suraj and Narayan, Avanika and Narayanan, Deepak and Newman, Ben and Nie, Allen and Niebles, Juan Carlos and Nilforoshan, Hamed and Nyarko, Julian and Ogut, Giray and Orr, Laurel and Papadimitriou, Isabel and Park, Joon Sung and Piech, Chris and Portelance, Eva and Potts, Christopher and Raghunathan, Aditi and Reich, Rob and Ren, Hongyu and Rong, Frieda and Roohani, Yusuf and Ruiz, Camilo and Ryan, Jack and Ré, Christopher and Sadigh, Dorsa and Sagawa, Shiori and Santhanam, Keshav and Shih, Andy and Srinivasan, Krishnan and Tamkin, Alex and Taori, Rohan and Thomas, Armin W. and Tramèr, Florian and Wang, Rose E. and Wang, William and Wu, Bohan and Wu, Jiajun and Wu, Yuhuai and Xie, Sang Michael and Yasunaga, Michihiro and You, Jiaxuan and Zaharia, Matei and Zhang, Michael and Zhang, Tianyi and Zhang, Xikun and Zhang, Yuhui and Zheng, Lucia and Zhou, Kaitlyn and Liang, Percy},
	urldate = {2025-10-09},
	date = {2022-07-12},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2108.07258 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Machine Learning},
	file = {PDF:/home/frieda/snap/zotero-snap/common/Zotero/storage/RNQWUN5T/Bommasani et al. - 2022 - On the Opportunities and Risks of Foundation Models.pdf:application/pdf},
}

@misc{chang_survey_2023,
	title = {A Survey on Evaluation of Large Language Models},
	url = {http://arxiv.org/abs/2307.03109},
	doi = {10.48550/arXiv.2307.03109},
	abstract = {Large language models ({LLMs}) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As {LLMs} continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine {LLMs} from various perspectives. This paper presents a comprehensive review of these evaluation methods for {LLMs}, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, educations, natural and social sciences, agent applications, and other areas. Secondly, we answer the `where' and `how' questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing performance of {LLMs}. Then, we summarize the success and failure cases of {LLMs} in different tasks. Finally, we shed light on several future challenges that lie ahead in {LLMs} evaluation. Our aim is to offer invaluable insights to researchers in the realm of {LLMs} evaluation, thereby aiding the development of more proficient {LLMs}. Our key point is that evaluation should be treated as an essential discipline to better assist the development of {LLMs}. We consistently maintain the related open-source materials at: https://github.com/{MLGroupJLU}/{LLM}-eval-survey.},
	number = {{arXiv}:2307.03109},
	publisher = {{arXiv}},
	author = {Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and Ye, Wei and Zhang, Yue and Chang, Yi and Yu, Philip S. and Yang, Qiang and Xie, Xing},
	urldate = {2025-10-09},
	date = {2023-12-29},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2307.03109 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {PDF:/home/frieda/snap/zotero-snap/common/Zotero/storage/SLNQEEFD/Chang et al. - 2023 - A Survey on Evaluation of Large Language Models.pdf:application/pdf},
}

@article{floridi_unified_2019,
	title = {A Unified Framework of Five Principles for {AI} in Society},
	url = {https://hdsr.mitpress.mit.edu/pub/l0jsh9d1},
	doi = {10.1162/99608f92.8cd550d1},
	abstract = {Artificial Intelligence ({AI}) is already having a major impact on society. As a result, many organizations have launched a wide range of initiatives to establish ethical principles for the adoption of socially beneficial {AI}. Unfortunately, the sheer volume of proposed principles threatens to overwhelm and confuse. How might this problem of ‘principle proliferation’ be solved? In this paper, we report the results of a fine-grained analysis of several of the highest-profile sets of ethical principles for {AI}. We assess whether these principles converge upon a set of agreed-upon principles, or diverge, with significant disagreement over what constitutes ‘ethical {AI}.’ Our analysis finds a high degree of overlap among the sets of principles we analyze. We then identify an overarching framework consisting of five core principles for ethical {AI}. Four of them are core principles commonly used in bioethics: beneficence, non-maleficence, autonomy, and justice. On the basis of our comparative analysis, we argue that a new principle is needed in addition: explicability, understood as incorporating both the epistemological sense of intelligibility (as an answer to the question ‘how does it work?’) and in the ethical sense of accountability (as an answer to the question: ‘who is responsible for the way it works?’). In the ensuing discussion, we note the limitations and assess the implications of this ethical framework for future efforts to create laws, rules, technical standards, and best practices for ethical {AI} in a wide range of contexts.},
	journaltitle = {Harvard Data Science Review},
	shortjournal = {Harvard Data Science Review},
	author = {Floridi, Luciano and Cowls, Josh},
	urldate = {2025-10-09},
	date = {2019-06-23},
	langid = {english},
	file = {PDF:/home/frieda/snap/zotero-snap/common/Zotero/storage/7W5D53TS/Floridi and Cowls - 2019 - A Unified Framework of Five Principles for AI in Society.pdf:application/pdf},
}

@inproceedings{knollmeyer_benchmarking_2024,
	location = {Porto, Portugal},
	title = {Benchmarking of Retrieval Augmented Generation: A Comprehensive Systematic Literature Review on Evaluation Dimensions, Evaluation Metrics and Datasets:},
	isbn = {978-989-758-716-0},
	url = {https://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0013065700003838},
	doi = {10.5220/0013065700003838},
	shorttitle = {Benchmarking of Retrieval Augmented Generation},
	abstract = {Despite the rapid advancements in the field of Large Language Models ({LLM}), traditional benchmarks have proven to be inadequate for assessing the performance of Retrieval Augmented Generation ({RAG}) systems. Therefore, this paper presents a comprehensive systematic literature review of evaluation dimensions, metrics, and datasets for {RAG} systems. This review identifies key evaluation dimensions such as context relevance, faithfulness, answer relevance, correctness, and citation quality. For each evaluation dimension, several metrics and evaluators are proposed on how to assess them. This paper synthesizes the findings from 12 relevant papers and presents a concept matrix that categorizes each evaluation approach. The results provide a foundation for the development of robust evaluation frameworks and suitable datasets that are essential for the effective implementation and deployment of {RAG} systems in real-world applications.},
	eventtitle = {16th International Conference on Knowledge Management and Information Systems},
	pages = {137--148},
	booktitle = {Proceedings of the 16th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management},
	publisher = {{SCITEPRESS} - Science and Technology Publications},
	author = {Knollmeyer, Simon and Caymazer, Oğuz and Koval, Leonid and Akmal, Muhammad and Asif, Saara and Mathias, Selvine and Großmann, Daniel},
	urldate = {2025-10-09},
	date = {2024},
	langid = {english},
	file = {PDF:/home/frieda/snap/zotero-snap/common/Zotero/storage/CQTDLAHT/Knollmeyer et al. - 2024 - Benchmarking of Retrieval Augmented Generation A Comprehensive Systematic Literature Review on Eval.pdf:application/pdf},
}

@article{watson_langchain_nodate,
	title = {Das {LangChain} und {LlamaIndex} Projects Lab Buch: Large Language Models für die Echte Welt},
	author = {Watson, Mark and {GmbH}, Mayflower},
	langid = {german},
	file = {PDF:/home/frieda/snap/zotero-snap/common/Zotero/storage/BGM6RMWE/Watson and GmbH - Das LangChain und LlamaIndex Projects Lab Buch Large Language Models für die Echte Welt.pdf:application/pdf},
}

@misc{liu_how_2017,
	title = {How {NOT} To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation},
	url = {http://arxiv.org/abs/1603.08023},
	doi = {10.48550/arXiv.1603.08023},
	shorttitle = {How {NOT} To Evaluate Your Dialogue System},
	abstract = {We investigate evaluation metrics for dialogue response generation systems where supervised labels, such as task completion, are not available. Recent works in response generation have adopted metrics from machine translation to compare a model’s generated response to a single target response. We show that these metrics correlate very weakly with human judgements in the non-technical Twitter domain, and not at all in the technical Ubuntu domain. We provide quantitative and qualitative results highlighting speciﬁc weaknesses in existing metrics, and provide recommendations for future development of better automatic evaluation metrics for dialogue systems.},
	number = {{arXiv}:1603.08023},
	publisher = {{arXiv}},
	author = {Liu, Chia-Wei and Lowe, Ryan and Serban, Iulian V. and Noseworthy, Michael and Charlin, Laurent and Pineau, Joelle},
	urldate = {2025-10-09},
	date = {2017-01-03},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1603.08023 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
	file = {PDF:/home/frieda/snap/zotero-snap/common/Zotero/storage/JVUWCVQB/Liu et al. - 2017 - How NOT To Evaluate Your Dialogue System An Empirical Study of Unsupervised Evaluation Metrics for.pdf:application/pdf},
}

@article{miller_explanation_2019,
	title = {Explanation in artificial intelligence: Insights from the social sciences},
	volume = {267},
	issn = {00043702},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0004370218305988},
	doi = {10.1016/j.artint.2018.07.007},
	shorttitle = {Explanation in artificial intelligence},
	pages = {1--38},
	journaltitle = {Artificial Intelligence},
	shortjournal = {Artificial Intelligence},
	author = {Miller, Tim},
	urldate = {2025-10-09},
	date = {2019-02},
	langid = {english},
	file = {PDF:/home/frieda/snap/zotero-snap/common/Zotero/storage/AHQRICND/Miller - 2019 - Explanation in artificial intelligence Insights from the social sciences.pdf:application/pdf},
}

@incollection{nonaka_knowledge-creating_1998,
	title = {The Knowledge-Creating Company},
	isbn = {978-0-7506-7009-8},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9780750670098500161},
	pages = {175--187},
	booktitle = {The Economic Impact of Knowledge},
	publisher = {Elsevier},
	author = {Nonaka, Ikujiro},
	urldate = {2025-10-09},
	date = {1998},
	langid = {english},
	doi = {10.1016/B978-0-7506-7009-8.50016-1},
	file = {PDF:/home/frieda/snap/zotero-snap/common/Zotero/storage/BMP7233K/Nonaka - 1998 - The Knowledge-Creating Company.pdf:application/pdf},
}

@misc{lewis_retrieval-augmented_2021,
	title = {Retrieval-Augmented Generation for Knowledge-Intensive {NLP} Tasks},
	url = {http://arxiv.org/abs/2005.11401},
	doi = {10.48550/arXiv.2005.11401},
	abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when ﬁne-tuned on downstream {NLP} tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-speciﬁc architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pretrained models with a differentiable access mechanism to explicit non-parametric memory have so far been only investigated for extractive downstream tasks. We explore a general-purpose ﬁne-tuning recipe for retrieval-augmented generation ({RAG}) — models which combine pre-trained parametric and non-parametric memory for language generation. We introduce {RAG} models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two {RAG} formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We ﬁne-tune and evaluate our models on a wide range of knowledgeintensive {NLP} tasks and set the state of the art on three open domain {QA} tasks, outperforming parametric seq2seq models and task-speciﬁc retrieve-and-extract architectures. For language generation tasks, we ﬁnd that {RAG} models generate more speciﬁc, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
	number = {{arXiv}:2005.11401},
	publisher = {{arXiv}},
	author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and Riedel, Sebastian and Kiela, Douwe},
	urldate = {2025-10-09},
	date = {2021-04-12},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2005.11401 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {PDF:/home/frieda/snap/zotero-snap/common/Zotero/storage/P84JEDBE/Lewis et al. - 2021 - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf:application/pdf},
}

@article{steidl_pipeline_2023,
	title = {The pipeline for the continuous development of artificial intelligence models—Current state of research and practice},
	volume = {199},
	issn = {01641212},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0164121223000109},
	doi = {10.1016/j.jss.2023.111615},
	abstract = {Companies struggle to continuously develop and deploy Artificial Intelligence ({AI}) models to complex production systems due to {AI} characteristics while assuring quality. To ease the development process, continuous pipelines for {AI} have become an active research area where consolidated and in-depth analysis regarding the terminology, triggers, tasks, and challenges is required.},
	pages = {111615},
	journaltitle = {Journal of Systems and Software},
	shortjournal = {Journal of Systems and Software},
	author = {Steidl, Monika and Felderer, Michael and Ramler, Rudolf},
	urldate = {2025-10-09},
	date = {2023-05},
	langid = {english},
	file = {PDF:/home/frieda/snap/zotero-snap/common/Zotero/storage/MGPC37HU/Steidl et al. - 2023 - The pipeline for the continuous development of artificial intelligence models—Current state of resea.pdf:application/pdf},
}

@misc{sun_head--tail_2024,
	title = {Head-to-Tail: How Knowledgeable are Large Language Models ({LLMs})? A.K.A. Will {LLMs} Replace Knowledge Graphs?},
	url = {http://arxiv.org/abs/2308.10168},
	doi = {10.48550/arXiv.2308.10168},
	shorttitle = {Head-to-Tail},
	abstract = {Since the recent prosperity of Large Language Models ({LLMs}), there have been interleaved discussions regarding how to reduce hallucinations from {LLM} responses, how to increase the factuality of {LLMs}, and whether Knowledge Graphs ({KGs}), which store the world knowledge in a symbolic form, will be replaced with {LLMs}. In this paper, we try to answer these questions from a new angle: How knowledgeable are {LLMs}? To answer this question, we constructed Headto-Tail, a benchmark that consists of 18K question-answer ({QA}) pairs regarding head, torso, and tail facts in terms of popularity. We designed an automated evaluation method and a set of metrics that closely approximate the knowledge an {LLM} confidently internalizes. Through a comprehensive evaluation of 16 publicly available {LLMs}, we show that existing {LLMs} are still far from being perfect in terms of their grasp of factual knowledge, especially for facts of torso-to-tail entities.},
	number = {{arXiv}:2308.10168},
	publisher = {{arXiv}},
	author = {Sun, Kai and Xu, Yifan Ethan and Zha, Hanwen and Liu, Yue and Dong, Xin Luna},
	urldate = {2025-10-09},
	date = {2024-04-03},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2308.10168 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {PDF:/home/frieda/snap/zotero-snap/common/Zotero/storage/Q2XNYYWP/Sun et al. - 2024 - Head-to-Tail How Knowledgeable are Large Language Models (LLMs) A.K.A. Will LLMs Replace Knowledge.pdf:application/pdf},
}

@misc{tao_are_2024,
	title = {Are {LLMs} Really Not Knowledgable? Mining the Submerged Knowledge in {LLMs}' Memory},
	url = {http://arxiv.org/abs/2412.20846},
	doi = {10.48550/arXiv.2412.20846},
	shorttitle = {Are {LLMs} Really Not Knowledgable?},
	abstract = {Large language models ({LLMs}) have shown promise as potential knowledge bases, yet they often struggle with question-answering tasks and are prone to hallucinations. While previous research attributes these issues to knowledge gaps in the model’s parameters, our investigation reveals a different phenomenon: {LLMs} often retain correct knowledge even when generating incorrect answers. Through analysis of model’s internal representations, we find that correct answers frequently appear among highprobability tokens despite not being selected as final outputs. Based on this observation, we introduce Hits@k, a new metric to assess knowledge retention independent of expression accuracy. Our extensive experiments demonstrate that {LLMs} store significantly more knowledge than their {QA} performance suggests. Building on these findings, we develop {SkipUnsure}, a method to improve answer accuracy by leveraging detected but unexpressed knowledge. Experiments on both open-domain and specificdomain datasets show consistent improvements, with accuracy gains of up to 11.8\% on {DBPedia} and 6.3\% on {IMDB}, without requiring model retraining.},
	number = {{arXiv}:2412.20846},
	publisher = {{arXiv}},
	author = {Tao, Xingjian and Wang, Yiwei and Cai, Yujun and Yang, Zhicheng and Tang, Jing},
	urldate = {2025-10-09},
	date = {2024-12-30},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2412.20846 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {PDF:/home/frieda/snap/zotero-snap/common/Zotero/storage/YHK8Z7VC/Tao et al. - 2024 - Are LLMs Really Not Knowledgable Mining the Submerged Knowledge in LLMs' Memory.pdf:application/pdf},
}

@online{noauthor_understanding_2019,
	title = {Understanding searches better than ever before},
	url = {https://blog.google/products/search/search-language-understanding-bert/},
	abstract = {How new advances in the science of language understanding will help you find more useful information in Search.},
	titleaddon = {Google},
	urldate = {2025-10-30},
	date = {2019-10-25},
	langid = {english},
	file = {Snapshot:/home/frieda/snap/zotero-snap/common/Zotero/storage/NEY5JD8H/search-language-understanding-bert.html:text/html},
}

@misc{ma_query_2023,
	title = {Query Rewriting for Retrieval-Augmented Large Language Models},
	url = {http://arxiv.org/abs/2305.14283},
	doi = {10.48550/arXiv.2305.14283},
	abstract = {Large Language Models ({LLMs}) play powerful, black-box readers in the retrieve-then-read pipeline, making remarkable progress in knowledge-intensive tasks. This work introduces a new framework, Rewrite-Retrieve-Read instead of the previous retrieve-then-read for the retrieval-augmented {LLMs} from the perspective of the query rewriting. Unlike prior studies focusing on adapting either the retriever or the reader, our approach pays attention to the adaptation of the search query itself, for there is inevitably a gap between the input text and the needed knowledge in retrieval. We first prompt an {LLM} to generate the query, then use a web search engine to retrieve contexts. Furthermore, to better align the query to the frozen modules, we propose a trainable scheme for our pipeline. A small language model is adopted as a trainable rewriter to cater to the black-box {LLM} reader. The rewriter is trained using the feedback of the {LLM} reader by reinforcement learning. Evaluation is conducted on downstream tasks, open-domain {QA} and multiple-choice {QA}. Experiments results show consistent performance improvement, indicating that our framework is proven effective and scalable, and brings a new framework for retrieval-augmented {LLM}.},
	number = {{arXiv}:2305.14283},
	publisher = {{arXiv}},
	author = {Ma, Xinbei and Gong, Yeyun and He, Pengcheng and Zhao, Hai and Duan, Nan},
	urldate = {2025-11-03},
	date = {2023-10-23},
	eprinttype = {arxiv},
	eprint = {2305.14283 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/home/frieda/snap/zotero-snap/common/Zotero/storage/Y7PCMFC7/Ma et al. - 2023 - Query Rewriting for Retrieval-Augmented Large Language Models.pdf:application/pdf;Snapshot:/home/frieda/snap/zotero-snap/common/Zotero/storage/MYRXBG23/2305.html:text/html},
}

@misc{gao_retrieval-augmented_2024,
	title = {Retrieval-Augmented Generation for Large Language Models: A Survey},
	url = {http://arxiv.org/abs/2312.10997},
	doi = {10.48550/arXiv.2312.10997},
	shorttitle = {Retrieval-Augmented Generation for Large Language Models},
	abstract = {Large Language Models ({LLMs}) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation ({RAG}) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. {RAG} synergistically merges {LLMs}' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of {RAG} paradigms, encompassing the Naive {RAG}, the Advanced {RAG}, and the Modular {RAG}. It meticulously scrutinizes the tripartite foundation of {RAG} frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in {RAG} systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.},
	number = {{arXiv}:2312.10997},
	publisher = {{arXiv}},
	author = {Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Meng and Wang, Haofen},
	urldate = {2025-11-03},
	date = {2024-03-27},
	eprinttype = {arxiv},
	eprint = {2312.10997 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/home/frieda/snap/zotero-snap/common/Zotero/storage/A6BZ36MI/Gao et al. - 2024 - Retrieval-Augmented Generation for Large Language Models A Survey.pdf:application/pdf;Snapshot:/home/frieda/snap/zotero-snap/common/Zotero/storage/GU7J2BEV/2312.html:text/html},
}

@misc{asai_self-rag_2023,
	title = {Self-{RAG}: Learning to Retrieve, Generate, and Critique through Self-Reflection},
	url = {http://arxiv.org/abs/2310.11511},
	doi = {10.48550/arXiv.2310.11511},
	shorttitle = {Self-{RAG}},
	abstract = {Despite their remarkable capabilities, large language models ({LLMs}) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation ({RAG}), an ad hoc approach that augments {LMs} with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes {LM} versatility or can lead to unhelpful response generation. We introduce a new framework called Self-Reflective Retrieval-Augmented Generation (Self-{RAG}) that enhances an {LM}'s quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary {LM} that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its own generations using special tokens, called reflection tokens. Generating reflection tokens makes the {LM} controllable during the inference phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that Self-{RAG} (7B and 13B parameters) significantly outperforms state-of-the-art {LLMs} and retrieval-augmented models on a diverse set of tasks. Specifically, Self-{RAG} outperforms {ChatGPT} and retrieval-augmented Llama2-chat on Open-domain {QA}, reasoning and fact verification tasks, and it shows significant gains in improving factuality and citation accuracy for long-form generations relative to these models.},
	number = {{arXiv}:2310.11511},
	publisher = {{arXiv}},
	author = {Asai, Akari and Wu, Zeqiu and Wang, Yizhong and Sil, Avirup and Hajishirzi, Hannaneh},
	urldate = {2025-11-03},
	date = {2023-10-17},
	eprinttype = {arxiv},
	eprint = {2310.11511 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Preprint PDF:/home/frieda/snap/zotero-snap/common/Zotero/storage/ZVWGTFRP/Asai et al. - 2023 - Self-RAG Learning to Retrieve, Generate, and Critique through Self-Reflection.pdf:application/pdf;Snapshot:/home/frieda/snap/zotero-snap/common/Zotero/storage/J6BXFQT5/2310.html:text/html},
}

@misc{mallen_when_2023,
	title = {When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories},
	url = {http://arxiv.org/abs/2212.10511},
	doi = {10.48550/arXiv.2212.10511},
	shorttitle = {When Not to Trust Language Models},
	abstract = {Despite their impressive performance on diverse tasks, large language models ({LMs}) still struggle with tasks requiring rich world knowledge, implying the limitations of relying solely on their parameters to encode a wealth of world knowledge. This paper aims to understand {LMs}' strengths and limitations in memorizing factual knowledge, by conducting large-scale knowledge probing experiments of 10 models and 4 augmentation methods on {PopQA}, our new open-domain {QA} dataset with 14k questions. We find that {LMs} struggle with less popular factual knowledge, and that scaling fails to appreciably improve memorization of factual knowledge in the long tail. We then show that retrieval-augmented {LMs} largely outperform orders of magnitude larger {LMs}, while unassisted {LMs} remain competitive in questions about high-popularity entities. Based on those findings, we devise a simple, yet effective, method for powerful and efficient retrieval-augmented {LMs}, which retrieves non-parametric memories only when necessary. Experimental results show that this significantly improves models' performance while reducing the inference costs.},
	number = {{arXiv}:2212.10511},
	publisher = {{arXiv}},
	author = {Mallen, Alex and Asai, Akari and Zhong, Victor and Das, Rajarshi and Khashabi, Daniel and Hajishirzi, Hannaneh},
	urldate = {2025-11-03},
	date = {2023-07-02},
	eprinttype = {arxiv},
	eprint = {2212.10511 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Preprint PDF:/home/frieda/snap/zotero-snap/common/Zotero/storage/BI4MXGBH/Mallen et al. - 2023 - When Not to Trust Language Models Investigating Effectiveness of Parametric and Non-Parametric Memo.pdf:application/pdf;Snapshot:/home/frieda/snap/zotero-snap/common/Zotero/storage/HQFN8VIF/2212.html:text/html},
}

@misc{zhang_sirens_2025,
	title = {Siren's Song in the {AI} Ocean: A Survey on Hallucination in Large Language Models},
	url = {http://arxiv.org/abs/2309.01219},
	doi = {10.48550/arXiv.2309.01219},
	shorttitle = {Siren's Song in the {AI} Ocean},
	abstract = {While large language models ({LLMs}) have demonstrated remarkable capabilities across a range of downstream tasks, a significant concern revolves around their propensity to exhibit hallucinations: {LLMs} occasionally generate content that diverges from the user input, contradicts previously generated context, or misaligns with established world knowledge. This phenomenon poses a substantial challenge to the reliability of {LLMs} in real-world scenarios. In this paper, we survey recent efforts on the detection, explanation, and mitigation of hallucination, with an emphasis on the unique challenges posed by {LLMs}. We present taxonomies of the {LLM} hallucination phenomena and evaluation benchmarks, analyze existing approaches aiming at mitigating {LLM} hallucination, and discuss potential directions for future research.},
	number = {{arXiv}:2309.01219},
	publisher = {{arXiv}},
	author = {Zhang, Yue and Li, Yafu and Cui, Leyang and Cai, Deng and Liu, Lemao and Fu, Tingchen and Huang, Xinting and Zhao, Enbo and Zhang, Yu and Xu, Chen and Chen, Yulong and Wang, Longyue and Luu, Anh Tuan and Bi, Wei and Shi, Freda and Shi, Shuming},
	urldate = {2025-11-03},
	date = {2025-09-14},
	eprinttype = {arxiv},
	eprint = {2309.01219 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Preprint PDF:/home/frieda/snap/zotero-snap/common/Zotero/storage/28NVL9VN/Zhang et al. - 2025 - Siren's Song in the AI Ocean A Survey on Hallucination in Large Language Models.pdf:application/pdf;Snapshot:/home/frieda/snap/zotero-snap/common/Zotero/storage/GM3QJJGF/2309.html:text/html},
}

@misc{jiang_active_2023,
	title = {Active Retrieval Augmented Generation},
	url = {http://arxiv.org/abs/2305.06983},
	doi = {10.48550/arXiv.2305.06983},
	abstract = {Despite the remarkable ability of large language models ({LMs}) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting {LMs} by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented {LMs} employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active {REtrieval} augmented generation ({FLARE}), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test {FLARE} along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. {FLARE} achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/{FLARE}.},
	number = {{arXiv}:2305.06983},
	publisher = {{arXiv}},
	author = {Jiang, Zhengbao and Xu, Frank F. and Gao, Luyu and Sun, Zhiqing and Liu, Qian and Dwivedi-Yu, Jane and Yang, Yiming and Callan, Jamie and Neubig, Graham},
	urldate = {2025-11-03},
	date = {2023-10-22},
	eprinttype = {arxiv},
	eprint = {2305.06983 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Preprint PDF:/home/frieda/snap/zotero-snap/common/Zotero/storage/3WLFHJF5/Jiang et al. - 2023 - Active Retrieval Augmented Generation.pdf:application/pdf;Snapshot:/home/frieda/snap/zotero-snap/common/Zotero/storage/QSTATUID/2305.html:text/html},
}

@software{mlgroupjlu_mlgroupjlullm-eval-survey_2025,
	title = {{MLGroupJLU}/{LLM}-eval-survey},
	url = {https://github.com/MLGroupJLU/LLM-eval-survey},
	abstract = {The official {GitHub} page for the survey paper "A Survey on Evaluation of Large Language Models".},
	author = {{MLGroupJLU}},
	urldate = {2025-11-06},
	date = {2025-11-05},
	note = {original-date: 2023-07-02T03:23:20Z},
	keywords = {benchmark, evaluation, large-language-models, llm, llms, model-assessment},
}

@online{noauthor_230701135_nodate,
	title = {[2307.01135] {ChatGPT} vs. Google: A Comparative Study of Search Performance and User Experience},
	url = {https://arxiv.org/abs/2307.01135},
	urldate = {2025-11-07},
	file = {[2307.01135] ChatGPT vs. Google\: A Comparative Study of Search Performance and User Experience:/home/frieda/snap/zotero-snap/common/Zotero/storage/FUB6UYDD/2307.html:text/html},
}

@misc{le_log_2023,
	title = {Log Parsing: How Far Can {ChatGPT} Go?},
	url = {http://arxiv.org/abs/2306.01590},
	doi = {10.48550/arXiv.2306.01590},
	shorttitle = {Log Parsing},
	abstract = {Software logs play an essential role in ensuring the reliability and maintainability of large-scale software systems, as they are often the sole source of runtime information. Log parsing, which converts raw log messages into structured data, is an important initial step towards downstream log analytics. In recent studies, {ChatGPT}, the current cutting-edge large language model ({LLM}), has been widely applied to a wide range of software engineering tasks. However, its performance in automated log parsing remains unclear. In this paper, we evaluate {ChatGPT}'s ability to undertake log parsing by addressing two research questions. (1) Can {ChatGPT} effectively parse logs? (2) How does {ChatGPT} perform with different prompting methods? Our results show that {ChatGPT} can achieve promising results for log parsing with appropriate prompts, especially with few-shot prompting. Based on our findings, we outline several challenges and opportunities for {ChatGPT}-based log parsing.},
	number = {{arXiv}:2306.01590},
	publisher = {{arXiv}},
	author = {Le, Van-Hoang and Zhang, Hongyu},
	urldate = {2025-11-07},
	date = {2023-08-20},
	eprinttype = {arxiv},
	eprint = {2306.01590 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
	file = {Preprint PDF:/home/frieda/snap/zotero-snap/common/Zotero/storage/5K3FY7VS/Le and Zhang - 2023 - Log Parsing How Far Can ChatGPT Go.pdf:application/pdf;Snapshot:/home/frieda/snap/zotero-snap/common/Zotero/storage/PJSKQ4U5/2306.html:text/html},
}

@misc{yu_kola_2024,
	title = {{KoLA}: Carefully Benchmarking World Knowledge of Large Language Models},
	url = {http://arxiv.org/abs/2306.09296},
	doi = {10.48550/arXiv.2306.09296},
	shorttitle = {{KoLA}},
	abstract = {The unprecedented performance of large language models ({LLMs}) necessitates improvements in evaluations. Rather than merely exploring the breadth of {LLM} abilities, we believe meticulous and thoughtful designs are essential to thorough, unbiased, and applicable evaluations. Given the importance of world knowledge to {LLMs}, we construct a Knowledge-oriented {LLM} Assessment benchmark ({KoLA}), in which we carefully design three crucial factors: (1) For {\textbackslash}textbf\{ability modeling\}, we mimic human cognition to form a four-level taxonomy of knowledge-related abilities, covering \$19\$ tasks. (2) For {\textbackslash}textbf\{data\}, to ensure fair comparisons, we use both Wikipedia, a corpus prevalently pre-trained by {LLMs}, along with continuously collected emerging corpora, aiming to evaluate the capacity to handle unseen data and evolving knowledge. (3) For {\textbackslash}textbf\{evaluation criteria\}, we adopt a contrastive system, including overall standard scores for better numerical comparability across tasks and models and a unique self-contrast metric for automatically evaluating knowledge-creating ability. We evaluate \$28\$ open-source and commercial {LLMs} and obtain some intriguing findings. The {KoLA} dataset and open-participation leaderboard are publicly released at https://kola.xlore.cn and will be continuously updated to provide references for developing {LLMs} and knowledge-related systems.},
	number = {{arXiv}:2306.09296},
	publisher = {{arXiv}},
	author = {Yu, Jifan and Wang, Xiaozhi and Tu, Shangqing and Cao, Shulin and Zhang-Li, Daniel and Lv, Xin and Peng, Hao and Yao, Zijun and Zhang, Xiaohan and Li, Hanming and Li, Chunyang and Zhang, Zheyuan and Bai, Yushi and Liu, Yantao and Xin, Amy and Lin, Nianyi and Yun, Kaifeng and Gong, Linlu and Chen, Jianhui and Wu, Zhili and Qi, Yunjia and Li, Weikai and Guan, Yong and Zeng, Kaisheng and Qi, Ji and Jin, Hailong and Liu, Jinxin and Gu, Yu and Yao, Yuan and Ding, Ning and Hou, Lei and Liu, Zhiyuan and Xu, Bin and Tang, Jie and Li, Juanzi},
	urldate = {2025-11-07},
	date = {2024-07-01},
	eprinttype = {arxiv},
	eprint = {2306.09296 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/home/frieda/snap/zotero-snap/common/Zotero/storage/8C4XTQVV/Yu et al. - 2024 - KoLA Carefully Benchmarking World Knowledge of Large Language Models.pdf:application/pdf;Snapshot:/home/frieda/snap/zotero-snap/common/Zotero/storage/HY5QGU8Z/2306.html:text/html},
}

@misc{liang_holistic_2023,
	title = {Holistic Evaluation of Language Models},
	url = {http://arxiv.org/abs/2211.09110},
	doi = {10.48550/arXiv.2211.09110},
	abstract = {Language models ({LMs}) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models ({HELM}) to improve the transparency of language models. First, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for {LMs}. Then we select a broad subset based on coverage and feasibility, noting what’s missing or underrepresented (e.g. question answering for neglected English dialects, metrics for trustworthiness). Second, we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios to the extent possible (87.5\% of the time), ensuring that metrics beyond accuracy don’t fall to the wayside, and that trade-offs across models and metrics are clearly exposed. We also perform 7 targeted evaluations, based on 26 targeted scenarios, to more deeply analyze specific aspects (e.g. knowledge, reasoning, memorization/copyright, disinformation). Third, we conduct a large-scale evaluation of 30 prominent language models (spanning open, limited-access, and closed models) on all 42 scenarios, including 21 scenarios that were not previously used in mainstream {LM} evaluation. Prior to {HELM}, models on average were evaluated on just 17.9\% of the core {HELM} scenarios, with some prominent models not sharing a single scenario in common. We improve this to 96.0\%: now all 30 models have been densely benchmarked on a set of core scenarios and metrics under standardized conditions. Our evaluation surfaces 25 top-level findings concerning the interplay between different scenarios, metrics, and models. For full transparency, we release all raw model prompts and completions publicly1 for further analysis, as well as a general modular toolkit for easily adding new scenarios, models, metrics, and prompting strategies.2 We intend for {HELM} to be a living benchmark for the community, continuously updated with new scenarios, metrics, and models.},
	number = {{arXiv}:2211.09110},
	publisher = {{arXiv}},
	author = {Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and Newman, Benjamin and Yuan, Binhang and Yan, Bobby and Zhang, Ce and Cosgrove, Christian and Manning, Christopher D. and Ré, Christopher and Acosta-Navas, Diana and Hudson, Drew A. and Zelikman, Eric and Durmus, Esin and Ladhak, Faisal and Rong, Frieda and Ren, Hongyu and Yao, Huaxiu and Wang, Jue and Santhanam, Keshav and Orr, Laurel and Zheng, Lucia and Yuksekgonul, Mert and Suzgun, Mirac and Kim, Nathan and Guha, Neel and Chatterji, Niladri and Khattab, Omar and Henderson, Peter and Huang, Qian and Chi, Ryan and Xie, Sang Michael and Santurkar, Shibani and Ganguli, Surya and Hashimoto, Tatsunori and Icard, Thomas and Zhang, Tianyi and Chaudhary, Vishrav and Wang, William and Li, Xuechen and Mai, Yifan and Zhang, Yuhui and Koreeda, Yuta},
	urldate = {2025-11-07},
	date = {2023-10-01},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2211.09110 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {PDF:/home/frieda/snap/zotero-snap/common/Zotero/storage/IMLTH5A7/Liang et al. - 2023 - Holistic Evaluation of Language Models.pdf:application/pdf},
}

@misc{wang_survey_2023,
	title = {Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity},
	url = {http://arxiv.org/abs/2310.07521},
	doi = {10.48550/arXiv.2310.07521},
	shorttitle = {Survey on Factuality in Large Language Models},
	abstract = {This survey addresses the crucial issue of factuality in Large Language Models ({LLMs}). As {LLMs} find applications across diverse domains, the reliability and accuracy of their outputs become vital. We define the Factuality Issue as the probability of {LLMs} to produce content inconsistent with established facts. We first delve into the implications of these inaccuracies, highlighting the potential consequences and challenges posed by factual errors in {LLM} outputs. Subsequently, we analyze the mechanisms through which {LLMs} store and process facts, seeking the primary causes of factual errors. Our discussion then transitions to methodologies for evaluating {LLM} factuality, emphasizing key metrics, benchmarks, and studies. We further explore strategies for enhancing {LLM} factuality, including approaches tailored for specific domains. We focus two primary {LLM} configurations standalone {LLMs} and Retrieval-Augmented {LLMs} that utilizes external data, we detail their unique challenges and potential enhancements. Our survey offers a structured guide for researchers aiming to fortify the factual reliability of {LLMs}.},
	number = {{arXiv}:2310.07521},
	publisher = {{arXiv}},
	author = {Wang, Cunxiang and Liu, Xiaoze and Yue, Yuanhao and Tang, Xiangru and Zhang, Tianhang and Jiayang, Cheng and Yao, Yunzhi and Gao, Wenyang and Hu, Xuming and Qi, Zehan and Wang, Yidong and Yang, Linyi and Wang, Jindong and Xie, Xing and Zhang, Zheng and Zhang, Yue},
	urldate = {2025-11-10},
	date = {2023-12-16},
	eprinttype = {arxiv},
	eprint = {2310.07521 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/home/frieda/snap/zotero-snap/common/Zotero/storage/BH8Q46TC/Wang et al. - 2023 - Survey on Factuality in Large Language Models Knowledge, Retrieval and Domain-Specificity.pdf:application/pdf;Snapshot:/home/frieda/snap/zotero-snap/common/Zotero/storage/EWRXPJLI/2310.html:text/html},
}

@misc{rackauckas_evaluating_2024,
	title = {Evaluating {RAG}-Fusion with {RAGElo}: an Automated Elo-based Framework},
	url = {http://arxiv.org/abs/2406.14783},
	doi = {10.48550/arXiv.2406.14783},
	shorttitle = {Evaluating {RAG}-Fusion with {RAGElo}},
	abstract = {Challenges in the automated evaluation of Retrieval-Augmented Generation ({RAG}) Question-Answering ({QA}) systems include hallucination problems in domain-specific knowledge and the lack of gold standard benchmarks for company internal tasks. This results in difficulties in evaluating {RAG} variations, like {RAG}-Fusion ({RAGF}), in the context of a product {QA} task at Infineon Technologies. To solve these problems, we propose a comprehensive evaluation framework, which leverages Large Language Models ({LLMs}) to generate large datasets of synthetic queries based on real user queries and in-domain documents, uses {LLM}-as-a-judge to rate retrieved documents and answers, evaluates the quality of answers, and ranks different variants of Retrieval-Augmented Generation ({RAG}) agents with {RAGElo}'s automated Elo-based competition. {LLM}-as-a-judge rating of a random sample of synthetic queries shows a moderate, positive correlation with domain expert scoring in relevance, accuracy, completeness, and precision. While {RAGF} outperformed {RAG} in Elo score, a significance analysis against expert annotations also shows that {RAGF} significantly outperforms {RAG} in completeness, but underperforms in precision. In addition, Infineon's {RAGF} assistant demonstrated slightly higher performance in document relevance based on {MRR}@5 scores. We find that {RAGElo} positively aligns with the preferences of human annotators, though due caution is still required. Finally, {RAGF}'s approach leads to more complete answers based on expert annotations and better answers overall based on {RAGElo}'s evaluation criteria.},
	number = {{arXiv}:2406.14783},
	publisher = {{arXiv}},
	author = {Rackauckas, Zackary and Câmara, Arthur and Zavrel, Jakub},
	urldate = {2025-11-10},
	date = {2024-10-08},
	eprinttype = {arxiv},
	eprint = {2406.14783 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {Preprint PDF:/home/frieda/snap/zotero-snap/common/Zotero/storage/FHJ8HIK7/Rackauckas et al. - 2024 - Evaluating RAG-Fusion with RAGElo an Automated Elo-based Framework.pdf:application/pdf;Snapshot:/home/frieda/snap/zotero-snap/common/Zotero/storage/E27JJ5CP/2406.html:text/html},
}

@incollection{yu_evaluation_2025,
	title = {Evaluation of Retrieval-Augmented Generation: A Survey},
	volume = {2301},
	url = {http://arxiv.org/abs/2405.07437},
	shorttitle = {Evaluation of Retrieval-Augmented Generation},
	abstract = {Retrieval-Augmented Generation ({RAG}) has recently gained traction in natural language processing. Numerous studies and real-world applications are leveraging its ability to enhance generative models through external information retrieval. Evaluating these {RAG} systems, however, poses unique challenges due to their hybrid structure and reliance on dynamic knowledge sources. To better understand these challenges, we conduct A Unified Evaluation Process of {RAG} (Auepora) and aim to provide a comprehensive overview of the evaluation and benchmarks of {RAG} systems. Specifically, we examine and compare several quantifiable metrics of the Retrieval and Generation components, such as relevance, accuracy, and faithfulness, within the current {RAG} benchmarks, encompassing the possible output and ground truth pairs. We then analyze the various datasets and metrics, discuss the limitations of current benchmarks, and suggest potential directions to advance the field of {RAG} benchmarks.},
	pages = {102--120},
	author = {Yu, Hao and Gan, Aoran and Zhang, Kai and Tong, Shiwei and Liu, Qi and Liu, Zhaofeng},
	urldate = {2025-11-10},
	date = {2025},
	doi = {10.1007/978-981-96-1024-2_8},
	eprinttype = {arxiv},
	eprint = {2405.07437 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/home/frieda/snap/zotero-snap/common/Zotero/storage/J8L3UFAP/Yu et al. - 2025 - Evaluation of Retrieval-Augmented Generation A Survey.pdf:application/pdf;Snapshot:/home/frieda/snap/zotero-snap/common/Zotero/storage/TPNSTJWW/2405.html:text/html},
}

@misc{es_ragas_2025,
	title = {Ragas: Automated Evaluation of Retrieval Augmented Generation},
	url = {http://arxiv.org/abs/2309.15217},
	doi = {10.48550/arXiv.2309.15217},
	shorttitle = {Ragas},
	abstract = {We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation ({RAG}) pipelines. {RAG} systems are composed of a retrieval and an {LLM} based generation module, and provide {LLMs} with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating {RAG} architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the {LLM} to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions {\textbackslash}textit\{without having to rely on ground truth human annotations\}. We posit that such a framework can crucially contribute to faster evaluation cycles of {RAG} architectures, which is especially important given the fast adoption of {LLMs}.},
	number = {{arXiv}:2309.15217},
	publisher = {{arXiv}},
	author = {Es, Shahul and James, Jithin and Espinosa-Anke, Luis and Schockaert, Steven},
	urldate = {2025-11-10},
	date = {2025-04-28},
	eprinttype = {arxiv},
	eprint = {2309.15217 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/home/frieda/snap/zotero-snap/common/Zotero/storage/FNT6P9EM/Es et al. - 2025 - Ragas Automated Evaluation of Retrieval Augmented Generation.pdf:application/pdf;Snapshot:/home/frieda/snap/zotero-snap/common/Zotero/storage/GJ7J74NN/2309.html:text/html},
}

@online{noauthor_open_2025,
	title = {Open {WebUI}},
	url = {https://openwebui.com/},
	abstract = {Open {WebUI} is an extensible, feature-rich, and user-friendly self-hosted {AI} platform designed to operate entirely offline. It supports various {LLM} runners like Ollama and {OpenAI}-compatible {APIs}, with built-in inference engine for {RAG}, making it a powerful {AI} deployment solution.},
	titleaddon = {{OpenWebUI}},
	type = {webpage},
	urldate = {2025-11-13},
	date = {2025-11-13},
	langid = {english},
	file = {Snapshot:/home/frieda/snap/zotero-snap/common/Zotero/storage/Y9R9N25L/openwebui.com.html:text/html},
}

@online{noauthor_gpt-oss-120b_nodate,
	title = {gpt-oss-120b Coding Evaluation: New Top Open-Source Model},
	url = {https://eval.16x.engineer/blog/gpt-oss-120b-coding-evaluation-results},
	shorttitle = {gpt-oss-120b Coding Evaluation},
	abstract = {Evaluation of gpt-oss-120b on coding tasks shows strong performance. We examine its performance across five coding challenges and compare it to top models.},
	titleaddon = {16x Eval},
	urldate = {2025-11-13},
	langid = {english},
	file = {Snapshot:/home/frieda/snap/zotero-snap/common/Zotero/storage/NU2EC69N/gpt-oss-120b-coding-evaluation-results.html:text/html},
}

@article{noauthor_designing_nodate,
	title = {Designing an open-source {LLM} interface and social platforms for collectively driven {LLM} evaluation and auditing},
	langid = {english},
	file = {PDF:/home/frieda/snap/zotero-snap/common/Zotero/storage/RTRZ3ABC/Designing an open-source LLM interface and social platforms for collectively driven LLM evaluation a.pdf:application/pdf},
}

@online{sui_table_2023,
	title = {Table Meets {LLM}: Can Large Language Models Understand Structured Table Data? A Benchmark and Empirical Study},
	url = {https://arxiv.org/abs/2305.13062v5},
	shorttitle = {Table Meets {LLM}},
	abstract = {Large language models ({LLMs}) are becoming attractive as few-shot reasoners to solve Natural Language ({NL})-related tasks. However, the understanding of their capability to process structured data like tables remains an under-explored area. While tables can be serialized as input for {LLMs}, there is a lack of comprehensive studies on whether {LLMs} genuinely comprehend this data. In this paper, we try to understand this by designing a benchmark to evaluate the structural understanding capabilities of {LLMs} through seven distinct tasks, e.g., cell lookup, row retrieval and size detection. Specially, we perform a series of evaluations on the recent most advanced {LLM} models, {GPT}-3.5 and {GPT}-4 and observe that performance varied with different input choices, including table input format, content order, role prompting, and partition marks. Drawing from the insights gained through the benchmark evaluations, we propose \${\textbackslash}textit\{self-augmentation\}\$ for effective structural prompting, such as critical value / range identification using internal knowledge of {LLMs}. When combined with carefully chosen input choices, these structural prompting methods lead to promising improvements in {LLM} performance on a variety of tabular tasks, e.g., {TabFact}(\${\textbackslash}uparrow2.31{\textbackslash}\%\$), {HybridQA}(\${\textbackslash}uparrow2.13{\textbackslash}\%\$), {SQA}(\${\textbackslash}uparrow2.72{\textbackslash}\%\$), Feverous(\${\textbackslash}uparrow0.84{\textbackslash}\%\$), and {ToTTo}(\${\textbackslash}uparrow5.68{\textbackslash}\%\$). We believe that our open source benchmark and proposed prompting methods can serve as a simple yet generic selection for future research. The code and data of this paper will be temporality released at https://anonymous.4open.science/r/{StructuredLLM}-76F3/{README}.md and will be replaced with an official one at https://github.com/microsoft/{TableProvider} later.},
	titleaddon = {{arXiv}.org},
	author = {Sui, Yuan and Zhou, Mengyu and Zhou, Mingjie and Han, Shi and Zhang, Dongmei},
	urldate = {2025-11-25},
	date = {2023-05-22},
	langid = {english},
	file = {Full Text PDF:/home/frieda/snap/zotero-snap/common/Zotero/storage/EI5LQ4KL/Sui et al. - 2023 - Table Meets LLM Can Large Language Models Understand Structured Table Data A Benchmark and Empiric.pdf:application/pdf},
}

@misc{wolff_how_2025,
	title = {How well do {LLMs} reason over tabular data, really?},
	url = {http://arxiv.org/abs/2505.07453},
	doi = {10.48550/arXiv.2505.07453},
	abstract = {Large Language Models ({LLMs}) excel in natural language tasks but less is known about their reasoning capabilities over tabular data. Prior analyses devise evaluation strategies that poorly reflect an {LLM}’s realistic performance on tabular queries. Moreover, we have a limited understanding of the robustness of {LLMs} towards realistic variations in tabular inputs. Therefore, we ask: can general-purpose {LLMs} reason over tabular data, really?, and focus on two questions 1) are tabular reasoning capabilities of general-purpose {LLMs} robust to real-world characteristics of tabular inputs, and 2) how can we realistically evaluate an {LLM}’s performance on analytical tabular queries? Building on a recent tabular reasoning benchmark, we first surface shortcomings of its multiple-choice prompt evaluation strategy, as well as commonly used free-form text metrics such as {SacreBleu} and {BERT}-score. We show that an {LLM}-as-a-judge procedure yields more reliable performance insights and unveil a significant deficit in tabular reasoning performance of {LLMs}. We then extend the tabular inputs reflecting three common characteristics in practice: 1) missing values, 2) duplicate entities, and 3) structural variations. Experiments show that the tabular reasoning capabilities of general-purpose {LLMs} suffer from these variations, stressing the importance of improving their robustness for realistic tabular inputs.},
	number = {{arXiv}:2505.07453},
	publisher = {{arXiv}},
	author = {Wolff, Cornelius and Hulsebos, Madelon},
	urldate = {2025-12-01},
	date = {2025-11-04},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2505.07453 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {PDF:/home/frieda/snap/zotero-snap/common/Zotero/storage/2QJXNSFD/Wolff and Hulsebos - 2025 - How well do LLMs reason over tabular data, really.pdf:application/pdf},
}

@misc{chen_open_2021,
	title = {Open Question Answering over Tables and Text},
	url = {http://arxiv.org/abs/2010.10439},
	doi = {10.48550/arXiv.2010.10439},
	abstract = {In open question answering ({QA}), the answer to a question is produced by retrieving and then analyzing documents that might contain answers to the question. Most open {QA} systems have considered only retrieving information from unstructured text. Here we consider for the first time open {QA} over both tabular and textual data and present a new large-scale dataset Open Table-and-Text Question Answering ({OTT}-{QA}) to evaluate performance on this task. Most questions in {OTT}-{QA} require multi-hop inference across tabular data and unstructured text, and the evidence required to answer a question can be distributed in different ways over these two types of input, making evidence retrieval challenging -- our baseline model using an iterative retriever and {BERT}-based reader achieves an exact match score less than 10\%. We then propose two novel techniques to address the challenge of retrieving and aggregating evidence for {OTT}-{QA}. The first technique is to use "early fusion" to group multiple highly relevant tabular and textual units into a fused block, which provides more context for the retriever to search for. The second technique is to use a cross-block reader to model the cross-dependency between multiple retrieved evidence with global-local sparse attention. Combining these two techniques improves the score significantly, to above 27\%.},
	number = {{arXiv}:2010.10439},
	publisher = {{arXiv}},
	author = {Chen, Wenhu and Chang, Ming-Wei and Schlinger, Eva and Wang, William and Cohen, William W.},
	urldate = {2025-12-01},
	date = {2021-02-10},
	eprinttype = {arxiv},
	eprint = {2010.10439 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/home/frieda/snap/zotero-snap/common/Zotero/storage/KKNCD5JC/Chen et al. - 2021 - Open Question Answering over Tables and Text.pdf:application/pdf;Snapshot:/home/frieda/snap/zotero-snap/common/Zotero/storage/CCERBRLV/2010.html:text/html},
}

@book{kimball_data_2013,
	location = {Erscheinungsort nicht ermittelbar},
	title = {The Data Warehouse Toolkit: The Definitive Guide to Dimensional Modeling},
	isbn = {978-1-118-53080-1},
	shorttitle = {The Data Warehouse Toolkit},
	abstract = {Updated new edition of Ralph Kimball's groundbreaking book on dimensional modeling for data warehousing and business intelligence!The first edition of Ralph Kimball's The Data Warehouse Toolkit introduced the industry to dimensional modeling,and now his books are considered the most authoritative guides in this space. This new third edition is a complete library of updated dimensional modeling techniques, the most comprehensive collection ever. It covers new and enhanced star schema dimensional modeling patterns, adds two new chapters on {ETL} techniques, includes new and expanded business matrices for 12 case studies, and more.Authored by Ralph Kimball and Margy Ross, known worldwide as educators, consultants, and influential thought leaders in data warehousing and business {intelligenceBegins} with fundamental design recommendations and progresses through increasingly complex {scenariosPresents} unique modeling techniques for business applications such as inventory management, procurement, invoicing, accounting,customer relationship management, big data analytics, and {moreDraws} real-world case studies from a variety of industries,including retail sales, financial services, telecommunications,education, health care, insurance, e-commerce, and {moreDesign} dimensional databases that are easy to understand and provide fast query response with The Data Warehouse Toolkit: The Definitive Guide to Dimensional Modeling, 3rd Edition.},
	pagetotal = {608},
	publisher = {Wiley},
	author = {Kimball, Ralph and Ross, Margy},
	date = {2013},
	file = {PDF:/home/frieda/snap/zotero-snap/common/Zotero/storage/ST972JEI/Kimball und Ross - The Data Warehouse Toolkit.pdf:application/pdf},
}

@article{inmon_building_nodate,
	title = {Building the Data Warehouse},
	author = {Inmon, W H},
	langid = {english},
	file = {PDF:/home/frieda/snap/zotero-snap/common/Zotero/storage/7L6VMZS8/Inmon - Building the Data Warehouse.pdf:application/pdf},
}

@online{noauthor_building_nodate,
	title = {Building the Data Warehouse: Inmon, W. H.: 8601300478937: Amazon.com: Books},
	url = {https://www.amazon.com/Building-Data-Warehouse-W-Inmon/dp/0764599445/ref=sr_1_1?crid=3ULG6O1RSGU51&dib=eyJ2IjoiMSJ9.vTC1G3nUNjI0v4PQDePBqNm7MLuoJ_j8gFrWOQ5qmkZoN2LF2kzO7uVPR6y9HBclWOKLm9lX2uxzTCyiS0bDVpmsUq-qz8ebT9ekr13VuE2vlxKE3s9a3RGB7-YRIKLwTTVcN42GdFw6sjrqeiPgTuUhUWwA1_-QGB_vfv4rS_vNRBoRKtmgbDoPv65YmbwimhOAybKAGYJu84_8jO54sQ.DVv0bNTZiDBgn0T8dKVXZDupz63LTghv-RuF1A2umaA&dib_tag=se&keywords=Inmon+%E2%80%93+Building+the+Data+Warehouse&qid=1765879138&s=books&sprefix=inmon+building+the+data+warehouse%2Cstripbooks%2C413&sr=1-1},
	urldate = {2025-12-16},
	file = {Building the Data Warehouse\: Inmon, W. H.\: 8601300478937\: Amazon.com\: Books:/home/frieda/snap/zotero-snap/common/Zotero/storage/XQV44XH4/ref=sr_1_1.html:text/html},
}

@article{zou_rag_2025,
	title = {{RAG} over Tables: Hierarchical Memory Index, Multi-Stage Retrieval, and Benchmarking},
	url = {https://openreview.net/forum?id=PifVwyqe5L&utm_source=chatgpt.com},
	shorttitle = {{RAG} over Tables},
	abstract = {Retrieval-Augmented Generation ({RAG}) enhances Large Language Models ({LLMs}) by integrating them with an external knowledge base to improve the answer relevance and accuracy. In real-world scenarios, beyond pure text, a substantial amount of knowledge is stored in tables, and user questions often require retrieving answers that are distributed across multiple tables. Retrieving knowledge from a table corpora (i.e., various individual tables) for a question remains nascent, at least, for (1) how to understand intra- and inter-table knowledge effectively, (2) how to filter unnecessary tables and how to retrieve the most relevant tables efficiently, (3) how to prompt {LLMs} to infer over the retrieval, (4) how to evaluate the corresponding performance in a realistic setting. Facing the above challenges, in this paper, we first propose a table-corpora-aware {RAG} framework, named T-{RAG}, which consists of the hierarchical memory index, multi-stage retrieval, and graph-aware prompting for effective and efficient table knowledge retrieval and inference. Further, we first develop a multi-table question answering benchmark named {MultiTableQA}, which spans 3 different task types, 57,193 tables, and 23,758 questions in total, and the sources are all from real-world scenarios. Based on {MultiTableQA}, we did the holistic comparison over table retrieval methods, {RAG} methods, and table-to-graph representation learning methods, where T-{RAG} shows the leading accuracy, recall, and running time performance. Also, under T-{RAG}, we evaluate the inference ability upgrade of different {LLMs}.},
	author = {Zou, Jiaru and Fu, Dongqi and Chen, Sirui and He, Xinrui and Li, Zihao and Zhu, Yada and Han, Jiawei and He, Jingrui},
	urldate = {2026-01-16},
	date = {2025-10-08},
	langid = {english},
	file = {Full Text PDF:/home/frieda/snap/zotero-snap/common/Zotero/storage/QCEL8X34/Zou et al. - 2025 - RAG over Tables Hierarchical Memory Index, Multi-Stage Retrieval, and Benchmarking.pdf:application/pdf},
}

@misc{yu_tablerag_2025,
	title = {{TableRAG}: A Retrieval Augmented Generation Framework for Heterogeneous Document Reasoning},
	url = {http://arxiv.org/abs/2506.10380},
	doi = {10.48550/arXiv.2506.10380},
	shorttitle = {{TableRAG}},
	abstract = {Retrieval-Augmented Generation ({RAG}) has demonstrated considerable effectiveness in open-domain question answering. However, when applied to heterogeneous documents, comprising both textual and tabular components, existing {RAG} approaches exhibit critical limitations. The prevailing practice of flattening tables and chunking strategies disrupts the intrinsic tabular structure, leads to information loss, and undermines the reasoning capabilities of {LLMs} in multi-hop, global queries. To address these challenges, we propose {TableRAG}, an {SQL}-based framework that unifies textual understanding and complex manipulations over tabular data. {TableRAG} iteratively operates in four steps: context-sensitive query decomposition, text retrieval, {SQL} programming and execution, and compositional intermediate answer generation. We also develop {HeteQA}, a novel benchmark designed to evaluate the multi-hop heterogeneous reasoning capabilities. Experimental results demonstrate that {TableRAG} consistently outperforms existing baselines on both public datasets and our {HeteQA}, establishing a new state-of-the-art for heterogeneous document question answering. We release {TableRAG} at https://github.com/yxh-y/{TableRAG}/tree/main.},
	number = {{arXiv}:2506.10380},
	publisher = {{arXiv}},
	author = {Yu, Xiaohan and Jian, Pu and Chen, Chong},
	urldate = {2026-01-16},
	date = {2025-09-30},
	eprinttype = {arxiv},
	eprint = {2506.10380 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {Preprint PDF:/home/frieda/snap/zotero-snap/common/Zotero/storage/G67WPB89/Yu et al. - 2025 - TableRAG A Retrieval Augmented Generation Framework for Heterogeneous Document Reasoning.pdf:application/pdf;Snapshot:/home/frieda/snap/zotero-snap/common/Zotero/storage/495IART8/2506.html:text/html},
}

@misc{parikh_totto_2020,
	title = {{ToTTo}: A Controlled Table-To-Text Generation Dataset},
	url = {http://arxiv.org/abs/2004.14373},
	doi = {10.48550/arXiv.2004.14373},
	shorttitle = {{ToTTo}},
	abstract = {We present {ToTTo}, an open-domain English table-to-text dataset with over 120,000 training examples that proposes a controlled generation task: given a Wikipedia table and a set of highlighted table cells, produce a one-sentence description. To obtain generated targets that are natural but also faithful to the source table, we introduce a dataset construction process where annotators directly revise existing candidate sentences from Wikipedia. We present systematic analyses of our dataset and annotation process as well as results achieved by several state-of-the-art baselines. While usually fluent, existing methods often hallucinate phrases that are not supported by the table, suggesting that this dataset can serve as a useful research benchmark for high-precision conditional text generation.},
	number = {{arXiv}:2004.14373},
	publisher = {{arXiv}},
	author = {Parikh, Ankur P. and Wang, Xuezhi and Gehrmann, Sebastian and Faruqui, Manaal and Dhingra, Bhuwan and Yang, Diyi and Das, Dipanjan},
	urldate = {2026-01-16},
	date = {2020-10-06},
	eprinttype = {arxiv},
	eprint = {2004.14373 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Preprint PDF:/home/frieda/snap/zotero-snap/common/Zotero/storage/WQVWABW4/Parikh et al. - 2020 - ToTTo A Controlled Table-To-Text Generation Dataset.pdf:application/pdf;Snapshot:/home/frieda/snap/zotero-snap/common/Zotero/storage/3S66XVW9/2004.html:text/html},
}

@misc{yin_tabert_2020,
	title = {{TaBERT}: Pretraining for Joint Understanding of Textual and Tabular Data},
	url = {http://arxiv.org/abs/2005.08314},
	doi = {10.48550/arXiv.2005.08314},
	shorttitle = {{TaBERT}},
	abstract = {Recent years have witnessed the burgeoning of pretrained language models ({LMs}) for text-based natural language ({NL}) understanding tasks. Such models are typically trained on free-form {NL} text, hence may not be suitable for tasks like semantic parsing over structured data, which require reasoning over both free-form {NL} questions and structured tabular data (e.g., database tables). In this paper we present {TaBERT}, a pretrained {LM} that jointly learns representations for {NL} sentences and (semi-)structured tables. {TaBERT} is trained on a large corpus of 26 million tables and their English contexts. In experiments, neural semantic parsers using {TaBERT} as feature representation layers achieve new best results on the challenging weakly-supervised semantic parsing benchmark {WikiTableQuestions}, while performing competitively on the text-to-{SQL} dataset Spider. Implementation of the model will be available at http://fburl.com/{TaBERT} .},
	number = {{arXiv}:2005.08314},
	publisher = {{arXiv}},
	author = {Yin, Pengcheng and Neubig, Graham and Yih, Wen-tau and Riedel, Sebastian},
	urldate = {2026-01-16},
	date = {2020-05-17},
	eprinttype = {arxiv},
	eprint = {2005.08314 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Preprint PDF:/home/frieda/snap/zotero-snap/common/Zotero/storage/PGYVMV87/Yin et al. - 2020 - TaBERT Pretraining for Joint Understanding of Textual and Tabular Data.pdf:application/pdf;Snapshot:/home/frieda/snap/zotero-snap/common/Zotero/storage/K9ZTDEKF/2005.html:text/html},
}

@misc{si_tabrag_2025,
	title = {{TabRAG}: Tabular Document Retrieval via Structured Language Representations},
	url = {http://arxiv.org/abs/2511.06582},
	doi = {10.48550/arXiv.2511.06582},
	shorttitle = {{TabRAG}},
	abstract = {Ingesting data for Retrieval-Augmented Generation ({RAG}) involves either fine-tuning the embedding model directly on the target corpus or parsing documents for embedding model encoding. The former, while accurate, incurs high computational hardware requirements, while the latter suffers from suboptimal performance when extracting tabular data. In this work, we address the latter by presenting {TabRAG}, a parsing-based {RAG} pipeline designed to tackle table-heavy documents via structured language representations. {TabRAG} outperforms existing popular parsing-based methods for generation and retrieval. Code is available at https://github.com/jacobyhsi/{TabRAG}.},
	number = {{arXiv}:2511.06582},
	publisher = {{arXiv}},
	author = {Si, Jacob and Qu, Mike and Lee, Michelle and Li, Yingzhen},
	urldate = {2026-01-26},
	date = {2025-11-10},
	eprinttype = {arxiv},
	eprint = {2511.06582 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Information Retrieval, Computer Science - Machine Learning},
	file = {Preprint PDF:/home/frieda/snap/zotero-snap/common/Zotero/storage/BGZSBJMH/Si et al. - 2025 - TabRAG Tabular Document Retrieval via Structured Language Representations.pdf:application/pdf;Snapshot:/home/frieda/snap/zotero-snap/common/Zotero/storage/LNJEDKBD/2511.html:text/html},
}

@inproceedings{devlin_bert_2019,
	location = {Minneapolis, Minnesota},
	title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	url = {https://aclanthology.org/N19-1423/},
	doi = {10.18653/v1/N19-1423},
	shorttitle = {{BERT}},
	abstract = {We introduce a new language representation model called {BERT}, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), {BERT} is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained {BERT} model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. {BERT} is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the {GLUE} score to 80.5 (7.7 point absolute improvement), {MultiNLI} accuracy to 86.7\% (4.6\% absolute improvement), {SQuAD} v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and {SQuAD} v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	eventtitle = {{NAACL}-{HLT} 2019},
	pages = {4171--4186},
	booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
	urldate = {2026-02-02},
	date = {2019-06},
	file = {Full Text PDF:/home/frieda/snap/zotero-snap/common/Zotero/storage/LISIUT6S/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf:application/pdf},
}
